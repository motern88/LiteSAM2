{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3b1c46-9f5c-41c1-9101-85db8709ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# Adapted from https://github.com/facebookresearch/sam2/blob/main/notebooks/video_predictor_example.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2",
   "metadata": {},
   "source": [
    "# Example of Efficient Track Anything for multiple objects tracking\n",
    "\n",
    "`build_efficienttam_video_predictor` for video segmentation and tracking. \n",
    "\n",
    "First install `efficient_track_anything` environment.\n",
    "\n",
    "You can also launch a local demo `python app.py` for track anything\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887b90f-6576-4ef8-964e-76d3a156ccb6",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/video_predictor_example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from efficient_track_anything.build_efficienttam import (\n",
    "    build_efficienttam_video_predictor,\n",
    ")\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(5)\n",
    "# `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n",
    "video_dir = \"./videos/bedroom\"\n",
    "\n",
    "# scan all the JPEG frame names in this directory\n",
    "frame_names = [\n",
    "    p\n",
    "    for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# take a look the first video frame\n",
    "frame_idx = 0\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# model\n",
    "checkpoint = \"../checkpoints/efficienttam_s.pt\"\n",
    "model_cfg = \"configs/efficienttam/efficienttam_s.yaml\"\n",
    "\n",
    "predictor = build_efficienttam_video_predictor(model_cfg, checkpoint, device=device)\n",
    "\n",
    "\n",
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels == 1]\n",
    "    neg_points = coords[labels == 0]\n",
    "    ax.scatter(\n",
    "        pos_points[:, 0],\n",
    "        pos_points[:, 1],\n",
    "        color=\"green\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        neg_points[:, 0],\n",
    "        neg_points[:, 1],\n",
    "        color=\"red\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
    "    )\n",
    "\n",
    "\n",
    "inference_state = predictor.init_state(video_path=video_dir)\n",
    "prompts = {}  # hold all the clicks we add for visualization\n",
    "\n",
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = (\n",
    "    2  # give a unique id to each object we interact with (it can be any integers)\n",
    ")\n",
    "\n",
    "# Let's add a positive click at (x, y) = (200, 300) to get started on the first object\n",
    "points = np.array([[200, 300]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)\n",
    "\n",
    "# add the first object\n",
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = (\n",
    "    2  # give a unique id to each object we interact with (it can be any integers)\n",
    ")\n",
    "\n",
    "# Let's add a 2nd negative click at (x, y) = (275, 175) to refine the first object\n",
    "# sending all clicks (and their labels) to `add_new_points_or_box`\n",
    "points = np.array([[200, 300], [275, 175]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1, 0], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)\n",
    "\n",
    "\n",
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = (\n",
    "    3  # give a unique id to each object we interact with (it can be any integers)\n",
    ")\n",
    "\n",
    "# Let's now move on to the second object we want to track (giving it object id `3`)\n",
    "# with a positive click at (x, y) = (400, 150)\n",
    "points = np.array([[400, 150]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "\n",
    "# `add_new_points_or_box` returns masks for all objects added so far on this interacted frame\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame on all objects\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)\n",
    "\n",
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(\n",
    "    inference_state\n",
    "):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)\n",
    "    fig.savefig(\"output_bedroom_frame_{}.png\".format(out_frame_idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
